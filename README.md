# DataVision - Plataforma de contenido interactivo

## Sobre este proyecto

Este repositorio forma parte de la **EspecializaciÃ³n en IngenierÃ­a de Datos** de [Dateneo](https://link.dateneo.com/web).
Durante el programa trabajamos con un caso realista y construimos un proyecto completo en AWS, aplicando ingesta, modelado, orquestaciÃ³n, gobierno y serving de datos.

La especializaciÃ³n incluye:

* FormaciÃ³n prÃ¡ctica con clases semanales en vivo.
* Acceso a un entorno en AWS para experimentar con herramientas reales.
* Proyecto final aplicable a tu portfolio profesional.

ConocÃ© mÃ¡s en [dateneo.com](https://link.dateneo.com/web).

## Sobre DataVision

DataVision es la empresa ficticia sobre la que trabajamos en toda la especializaciÃ³n. Es una plataforma en lÃ­nea que ofrece la posibilidad de crear contenido interactivo en diversos formatos, como artÃ­culos, lÃ­neas de tiempo, cuestionarios y proyectos prÃ¡cticos. Los usuarios se registran con cuentas gratuitas o de pago (suscripciones Premium o Empresarial), lo que determina el acceso y la cantidad de contenidos que pueden generar mensualmente.

### Modelo de negocio

- **Suscripciones freemium**: con planes gratuitos, Premium y Empresarial.
- **Contenido interactivo**: diversificado (artÃ­culos, lÃ­neas de tiempo, cuestionarios, proyectos).
- **LÃ­mites por plan**: que determinan la cantidad de contenido generable mensualmente.
- **Experiencia personalizada**: basada en el tipo de usuario y sus interacciones.
- **CampaÃ±as de marketing**: dirigidas para conversiÃ³n y retenciÃ³n.

### Necesidad analÃ­tica

DataVision necesita transformar los datos operacionales en insights accionables para:

- **Segmentar usuarios**: segÃºn su comportamiento (RFM) para estrategias de retenciÃ³n y conversiÃ³n.
- **Predecir churn**: y diseÃ±ar intervenciones proactivas para usuarios Premium/Empresarial.
- **Optimizar recomendaciones**: de contenido interactivo para aumentar engagement.
- **Automatizar campaÃ±as**: de marketing basadas en segmentaciÃ³n y lÃ­mites de uso.
- **Monitorear mÃ©tricas**: de negocio en tiempo real (conversiones, uso de contenido, retenciÃ³n).

## Arquitectura del proyecto

Este proyecto implementa una **arquitectura lakehouse de tres capas** que procesa datos desde sistemas transaccionales hasta insights de negocio:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Layer     â”‚    â”‚  Staging Layer  â”‚    â”‚ Analytics Layer â”‚
â”‚   (S3 Raw)      â”‚â”€â”€â”€â–¶â”‚   (S3 Staging)  â”‚â”€â”€â”€â–¶â”‚  (S3 Analytics) â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Datos crudos  â”‚    â”‚ â€¢ Datos limpios â”‚    â”‚ â€¢ Tablas dims   â”‚
â”‚ â€¢ Sin estructuraâ”‚    â”‚ â€¢ Normalizados  â”‚    â”‚ â€¢ Tablas facts  â”‚
â”‚ â€¢ HistÃ³rico     â”‚    â”‚ â€¢ Validados     â”‚    â”‚ â€¢ MÃ©tricas RFM  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## MÃ³dulos del proyecto

### ğŸ”„ [Ingesta](ingesta/README.md)
**Problema**: capturar datos de mÃºltiples fuentes (sistemas transaccionales, APIs, logs) de forma confiable y escalable.

**SoluciÃ³n**: pipeline ELT usando dltHub que extrae, normaliza y carga datos al lakehouse, con validaciones de calidad integradas.

**TecnologÃ­as**: Python, dltHub, AWS Athena, S3, DuckDB, Docker.

### ğŸ”„ [TransformaciÃ³n](transformacion/README.md)
**Problema**: convertir datos crudos en modelos dimensionales para anÃ¡lisis de negocio.

**SoluciÃ³n**: modelo dimensional implementado con dbt que construye tablas de dimensiones y hechos, incluyendo segmentaciÃ³n RFM y anÃ¡lisis de churn.

**TecnologÃ­as**: dbt, SQL, AWS Athena, S3, Docker.

### ğŸ”„ [OrquestaciÃ³n](orquestacion/README.md)
**Problema**: coordinar la ejecuciÃ³n de pipelines complejos con dependencias, manejo de errores y monitoreo.

**SoluciÃ³n**: orquestaciÃ³n con Apache Airflow que ejecuta contenedores Docker en AWS Fargate, con retry automÃ¡tico y alertas.

**TecnologÃ­as**: Apache Airflow, AWS ECS Fargate, ECR, CloudWatch.

### ğŸ”„ [Calidad de datos](calidad_de_datos/README.md)
**Problema**: garantizar la confiabilidad de los datos desde la ingesta hasta el consumo.

**SoluciÃ³n**: tests automatizados con Soda Core que validan integridad, completitud y reglas de negocio.

**TecnologÃ­as**: Soda Core, SodaCL, Python, integraciÃ³n con dbt.

### ğŸ”„ [Consumo y Serving](consumo_y_serving/README.md)
**Problema**: exponer datos analÃ­ticos a equipos de negocio y sistemas operativos.

**SoluciÃ³n**: API REST para consultas en tiempo real y reverse ETL para sincronizaciÃ³n con CRM.

**TecnologÃ­as**: FastAPI, AWS Lambda, API Gateway, HubSpot API.

### ğŸ”„ [NivelaciÃ³n](nivelacion/README.md)
**Problema**: repasar conceptos fundamentales de Python y SQL antes de implementar arquitecturas complejas.

**SoluciÃ³n**: ETL bÃ¡sico que automatiza reportes de Excel usando solo Python y SQL.

**TecnologÃ­as**: Python, SQL, pandas, openpyxl.

### ğŸ”„ [CI/CD](.github/workflows/README.md)
**Problema**: automatizar pruebas, builds y despliegues para mantener la calidad y confiabilidad del cÃ³digo.

**SoluciÃ³n**: pipelines de GitHub Actions que ejecutan tests, construyen imÃ¡genes Docker y despliegan automÃ¡ticamente a AWS.

**TecnologÃ­as**: GitHub Actions, Docker, AWS ECR, ECS Fargate.

## Flujo de datos end-to-end

1. **Ingesta**: datos de sistemas transaccionales se extraen y cargan en S3 (raw layer)
2. **TransformaciÃ³n**: datos se limpian, normalizan y modelan dimensionalmente (staging + analytics layers)
3. **Calidad**: tests automatizados validan integridad y reglas de negocio
4. **OrquestaciÃ³n**: pipelines se ejecutan de forma programada con monitoreo
5. **Consumo**: datos se exponen via API para dashboards y se sincronizan con CRM

## Arquitectura tÃ©cnica

![Arquitectura de datos](Arquitectura_de_datos.jpg)

## TecnologÃ­as principales

- **Cloud**: AWS (S3, Athena, Lambda, ECS Fargate, ECR, API Gateway)
- **OrquestaciÃ³n**: Apache Airflow
- **TransformaciÃ³n**: dbt (Data Build Tool)
- **Ingesta**: dltHub, Python
- **Calidad**: Soda Core
- **API**: FastAPI, Mangum
- **Contenedores**: Docker
- **Monitoreo**: CloudWatch, Airflow UI

## Estructura del repositorio

```
especializacion-ingenieria-de-datos/
â”œâ”€â”€ ingesta/                    # MÃ³dulo de ingesta de datos
â”œâ”€â”€ transformacion/             # MÃ³dulo de transformaciÃ³n con dbt
â”œâ”€â”€ orquestacion/              # MÃ³dulo de orquestaciÃ³n con Airflow
â”œâ”€â”€ calidad_de_datos/          # MÃ³dulo de calidad de datos
â”œâ”€â”€ consumo_y_serving/         # MÃ³dulo de consumo (API + Reverse ETL)
â”œâ”€â”€ nivelacion/                # MÃ³dulo de nivelaciÃ³n (ETL bÃ¡sico)
â”œâ”€â”€ .github/workflows/         # Pipelines de CI/CD con GitHub Actions
```

## Casos de uso implementados

### AnÃ¡lisis RFM
- **SegmentaciÃ³n de usuarios**: basada en recency, frequency y monetary value.
- **IdentificaciÃ³n de usuarios**: en riesgo de churn (especialmente Premium/Empresarial).
- **Estrategias de retenciÃ³n**: personalizadas segÃºn el plan de suscripciÃ³n.

### AutomatizaciÃ³n de marketing
- **SincronizaciÃ³n de segmentos RFM**: con HubSpot CRM.

### API de datos
- **Consultas en tiempo real**: de segmentaciÃ³n de usuarios
